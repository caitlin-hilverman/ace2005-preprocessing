{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "\n",
    "#sys.path.append('/Users/chilv/Documents/proj-wm/event_extraction/bert-event-extraction-master/ace2005-preprocessing-master')\n",
    "\n",
    "from tqdm import tqdm\n",
    "# from main import find_token_index\n",
    "# from _parser import Parser\n",
    "# import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg', disable=[\"tagger\", \"parser\"])\n",
    "max_length = os.getenv(\"MAX_DOCUMENT_LENGTH\")\n",
    "if max_length:\n",
    "    nlp.max_length = int(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "globbed_files = glob.glob(\"/Users/chilv/Documents/proj-wm/bias-stance/bias_stance/MITRE Six-Twelve Month and November Docs CDRs/*.cdr\")\n",
    "data = []\n",
    "for one_file in globbed_files:\n",
    "    frame = pd.read_json(one_file, lines=True)\n",
    "    data.append(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdr_data = pd.concat(data, ignore_index = True, sort = False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = cdr_data['extracted_text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "nlp.add_pipe(sentencizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"caitie is providing a couple of sample sentences. caitie might need some help from jewell or max.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "caitie is providing a couple of sample sentences. caitie might need some help from jewell or max."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_dict_list(doc):\n",
    "    \"\"\"Returns a list of dictionaries for each sentence in a CDR.\n",
    "    This is just a few of those necessary for the model.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    for sent in doc.sents:\n",
    "        sentence_dict = {}\n",
    "        sentence_dict['sentence'] = sent.text\n",
    "        sentence_dict['position'] = [sent.start_char, sent.end_char]\n",
    "        entities = []\n",
    "        for ent in sent.ents:\n",
    "            entity_dict = {}\n",
    "            entity_dict['text'] = ent.text\n",
    "            entity_dict['position'] = [ent.start_char, ent.end_char]\n",
    "            entity_dict['entity-type'] = ent.label_\n",
    "            entities.append(entity_dict)\n",
    "        sentence_dict['golden-entity-mentions'] = entities\n",
    "        sentence_dict['golden-event-mentions'] = []\n",
    "        sentences.append(sentence_dict)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence': 'caitie is providing a couple of sample sentences.',\n",
       "  'position': [0, 49],\n",
       "  'golden-entity-mentions': [{'text': 'caitie',\n",
       "    'position': [0, 6],\n",
       "    'entity-type': 'ORG'}],\n",
       "  'golden-event-mentions': []},\n",
       " {'sentence': 'caitie might need some help from jewell or max.',\n",
       "  'position': [50, 97],\n",
       "  'golden-entity-mentions': [{'text': 'caitie',\n",
       "    'position': [50, 56],\n",
       "    'entity-type': 'PERSON'},\n",
       "   {'text': 'jewell', 'position': [83, 89], 'entity-type': 'PERSON'},\n",
       "   {'text': 'max', 'position': [93, 96], 'entity-type': 'PERSON'}],\n",
       "  'golden-event-mentions': []}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sentence_dict_list(doc)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StanfordNLP:\n",
    "    \"\"\"Getting Stanford running with necessary annotators\"\"\"\n",
    "    def __init__(self, host='http://localhost', port=9000):\n",
    "        self.nlp = StanfordCoreNLP(host, port=port,\n",
    "                                   timeout=60000)  # , quiet=False, logging_level=logging.DEBUG)\n",
    "        self.props = {\n",
    "            'annotators': 'tokenize,ssplit,pos,lemma,parse',\n",
    "            'pipelineLanguage': 'en',\n",
    "            'outputFormat': 'json'\n",
    "        }\n",
    "\n",
    "    def annotate(self, sentence):\n",
    "        return self.nlp.annotate(sentence, properties=self.props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Snlp = StanfordNLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stanford_core_data(sentences):\n",
    "    \"\"\"Fills in the stanford core values needed for the model.\"\"\"\n",
    "    result = []\n",
    "    for item in sentences:\n",
    "        data = dict()\n",
    "        data['sentence'] = item['sentence']\n",
    "        data['golden-entity-mentions'] = item['golden-entity-mentions']\n",
    "        data['golden-event-mentions'] = []\n",
    "        try:\n",
    "            nlp_res_raw = Snlp.annotate(item['sentence'])\n",
    "            nlp_res = json.loads(nlp_res_raw)\n",
    "            result.append(data)\n",
    "        except Exception as e:\n",
    "            print('[Warning] StanfordCore Exception: ', nlp_res_raw, 'This sentence will be ignored.')\n",
    "            print('If you want to include all sentences, please refer to this issue: https://github.com/nlpcl-lab/ace2005-preprocessing/issues/1')\n",
    "            continue\n",
    "        tokens = nlp_res['sentences'][0]['tokens']\n",
    "        data['stanford-colcc'] = []\n",
    "        for dep in nlp_res['sentences'][0]['enhancedPlusPlusDependencies']:\n",
    "            data['stanford-colcc'].append('{}/dep={}/gov={}'.format(dep['dep'], dep['dependent'] - 1, dep['governor'] - 1))\n",
    "\n",
    "        data['words'] = list(map(lambda x: x['word'], tokens))\n",
    "        data['pos-tags'] = list(map(lambda x: x['pos'], tokens))\n",
    "        data['lemma'] = list(map(lambda x: x['lemma'], tokens))\n",
    "        data['parse'] = nlp_res['sentences'][0]['parse']\n",
    "        result.append(data)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence': 'caitie is providing a couple of sample sentences.',\n",
       "  'golden-entity-mentions': [{'text': 'caitie',\n",
       "    'position': [0, 6],\n",
       "    'entity-type': 'ORG'}],\n",
       "  'golden-event-mentions': [],\n",
       "  'stanford-colcc': ['ROOT/dep=2/gov=-1',\n",
       "   'nsubj/dep=0/gov=2',\n",
       "   'aux/dep=1/gov=2',\n",
       "   'det:qmod/dep=3/gov=7',\n",
       "   'mwe/dep=4/gov=3',\n",
       "   'mwe/dep=5/gov=3',\n",
       "   'compound/dep=6/gov=7',\n",
       "   'dobj/dep=7/gov=2',\n",
       "   'punct/dep=8/gov=2'],\n",
       "  'words': ['caitie',\n",
       "   'is',\n",
       "   'providing',\n",
       "   'a',\n",
       "   'couple',\n",
       "   'of',\n",
       "   'sample',\n",
       "   'sentences',\n",
       "   '.'],\n",
       "  'pos-tags': ['NN', 'VBZ', 'VBG', 'DT', 'NN', 'IN', 'NN', 'NNS', '.'],\n",
       "  'lemma': ['caitie',\n",
       "   'be',\n",
       "   'provide',\n",
       "   'a',\n",
       "   'couple',\n",
       "   'of',\n",
       "   'sample',\n",
       "   'sentence',\n",
       "   '.'],\n",
       "  'parse': '(ROOT\\n  (S\\n    (NP (NN caitie))\\n    (VP (VBZ is)\\n      (VP (VBG providing)\\n        (NP\\n          (NP (DT a) (NN couple))\\n          (PP (IN of)\\n            (NP (NN sample) (NNS sentences))))))\\n    (. .)))'},\n",
       " {'sentence': 'caitie is providing a couple of sample sentences.',\n",
       "  'golden-entity-mentions': [{'text': 'caitie',\n",
       "    'position': [0, 6],\n",
       "    'entity-type': 'ORG'}],\n",
       "  'golden-event-mentions': [],\n",
       "  'stanford-colcc': ['ROOT/dep=2/gov=-1',\n",
       "   'nsubj/dep=0/gov=2',\n",
       "   'aux/dep=1/gov=2',\n",
       "   'det:qmod/dep=3/gov=7',\n",
       "   'mwe/dep=4/gov=3',\n",
       "   'mwe/dep=5/gov=3',\n",
       "   'compound/dep=6/gov=7',\n",
       "   'dobj/dep=7/gov=2',\n",
       "   'punct/dep=8/gov=2'],\n",
       "  'words': ['caitie',\n",
       "   'is',\n",
       "   'providing',\n",
       "   'a',\n",
       "   'couple',\n",
       "   'of',\n",
       "   'sample',\n",
       "   'sentences',\n",
       "   '.'],\n",
       "  'pos-tags': ['NN', 'VBZ', 'VBG', 'DT', 'NN', 'IN', 'NN', 'NNS', '.'],\n",
       "  'lemma': ['caitie',\n",
       "   'be',\n",
       "   'provide',\n",
       "   'a',\n",
       "   'couple',\n",
       "   'of',\n",
       "   'sample',\n",
       "   'sentence',\n",
       "   '.'],\n",
       "  'parse': '(ROOT\\n  (S\\n    (NP (NN caitie))\\n    (VP (VBZ is)\\n      (VP (VBG providing)\\n        (NP\\n          (NP (DT a) (NN couple))\\n          (PP (IN of)\\n            (NP (NN sample) (NNS sentences))))))\\n    (. .)))'},\n",
       " {'sentence': 'caitie might need some help from jewell or max.',\n",
       "  'golden-entity-mentions': [{'text': 'caitie',\n",
       "    'position': [50, 56],\n",
       "    'entity-type': 'PERSON'},\n",
       "   {'text': 'jewell', 'position': [83, 89], 'entity-type': 'PERSON'},\n",
       "   {'text': 'max', 'position': [93, 96], 'entity-type': 'PERSON'}],\n",
       "  'golden-event-mentions': [],\n",
       "  'stanford-colcc': ['ROOT/dep=2/gov=-1',\n",
       "   'nsubj/dep=0/gov=2',\n",
       "   'aux/dep=1/gov=2',\n",
       "   'det/dep=3/gov=4',\n",
       "   'dobj/dep=4/gov=2',\n",
       "   'case/dep=5/gov=6',\n",
       "   'nmod:from/dep=6/gov=2',\n",
       "   'cc/dep=7/gov=6',\n",
       "   'nmod:from/dep=8/gov=2',\n",
       "   'conj:or/dep=8/gov=6',\n",
       "   'punct/dep=9/gov=2'],\n",
       "  'words': ['caitie',\n",
       "   'might',\n",
       "   'need',\n",
       "   'some',\n",
       "   'help',\n",
       "   'from',\n",
       "   'jewell',\n",
       "   'or',\n",
       "   'max',\n",
       "   '.'],\n",
       "  'pos-tags': ['NN', 'MD', 'VB', 'DT', 'NN', 'IN', 'NN', 'CC', 'NN', '.'],\n",
       "  'lemma': ['caitie',\n",
       "   'might',\n",
       "   'need',\n",
       "   'some',\n",
       "   'help',\n",
       "   'from',\n",
       "   'jewell',\n",
       "   'or',\n",
       "   'max',\n",
       "   '.'],\n",
       "  'parse': '(ROOT\\n  (S\\n    (NP (NN caitie))\\n    (VP (MD might)\\n      (VP (VB need)\\n        (NP (DT some) (NN help))\\n        (PP (IN from)\\n          (NP (NN jewell)\\n            (CC or)\\n            (NN max)))))\\n    (. .)))'},\n",
       " {'sentence': 'caitie might need some help from jewell or max.',\n",
       "  'golden-entity-mentions': [{'text': 'caitie',\n",
       "    'position': [50, 56],\n",
       "    'entity-type': 'PERSON'},\n",
       "   {'text': 'jewell', 'position': [83, 89], 'entity-type': 'PERSON'},\n",
       "   {'text': 'max', 'position': [93, 96], 'entity-type': 'PERSON'}],\n",
       "  'golden-event-mentions': [],\n",
       "  'stanford-colcc': ['ROOT/dep=2/gov=-1',\n",
       "   'nsubj/dep=0/gov=2',\n",
       "   'aux/dep=1/gov=2',\n",
       "   'det/dep=3/gov=4',\n",
       "   'dobj/dep=4/gov=2',\n",
       "   'case/dep=5/gov=6',\n",
       "   'nmod:from/dep=6/gov=2',\n",
       "   'cc/dep=7/gov=6',\n",
       "   'nmod:from/dep=8/gov=2',\n",
       "   'conj:or/dep=8/gov=6',\n",
       "   'punct/dep=9/gov=2'],\n",
       "  'words': ['caitie',\n",
       "   'might',\n",
       "   'need',\n",
       "   'some',\n",
       "   'help',\n",
       "   'from',\n",
       "   'jewell',\n",
       "   'or',\n",
       "   'max',\n",
       "   '.'],\n",
       "  'pos-tags': ['NN', 'MD', 'VB', 'DT', 'NN', 'IN', 'NN', 'CC', 'NN', '.'],\n",
       "  'lemma': ['caitie',\n",
       "   'might',\n",
       "   'need',\n",
       "   'some',\n",
       "   'help',\n",
       "   'from',\n",
       "   'jewell',\n",
       "   'or',\n",
       "   'max',\n",
       "   '.'],\n",
       "  'parse': '(ROOT\\n  (S\\n    (NP (NN caitie))\\n    (VP (MD might)\\n      (VP (VB need)\\n        (NP (DT some) (NN help))\\n        (PP (IN from)\\n          (NP (NN jewell)\\n            (CC or)\\n            (NN max)))))\\n    (. .)))'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "almost_there = get_stanford_core_data(sentences)\n",
    "almost_there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_token_index(sentences):\n",
    "    result = []\n",
    "    start_idx, end_idx = -1, -1\n",
    "    for item in sentences:\n",
    "        data = item   \n",
    "        nlp_res_raw = Snlp.annotate(item['sentence'])\n",
    "        nlp_res = json.loads(nlp_res_raw)\n",
    "        tokens = nlp_res['sentences'][0]['tokens']\n",
    "        for entity_mention in item['golden-entity-mentions']:\n",
    "            if tokens['characterOffsetBegin'] == entity_mention['position'][0]:\n",
    "                entity_mention['start'] = tokens['index']\n",
    "            else:\n",
    "                print(\"\")                \n",
    "                \n",
    "        result.append(data)\n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-e45826b892fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_entity_token_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malmost_there\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-119-6de86cbf41f9>\u001b[0m in \u001b[0;36mget_entity_token_index\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mentity_mention\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'golden-entity-mentions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'characterOffsetBegin'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mentity_mention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'position'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                 \u001b[0mentity_mention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "get_entity_token_index(almost_there)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-119-6de86cbf41f9>\u001b[0m(14)\u001b[0;36mget_entity_token_index\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     12 \u001b[0;31m        \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     13 \u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0mentity_mention\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'golden-entity-mentions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 14 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'characterOffsetBegin'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mentity_mention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'position'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m                \u001b[0mentity_mention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     16 \u001b[0;31m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> item['golden-entity-mentions']\n",
      "[{'text': 'caitie', 'position': [0, 6], 'entity-type': 'ORG'}]\n",
      "ipdb> tokens['characterOffsetBegin']\n",
      "*** TypeError: list indices must be integers or slices, not str\n",
      "ipdb> tokens\n",
      "[{'index': 1, 'word': 'caitie', 'originalText': 'caitie', 'lemma': 'caitie', 'characterOffsetBegin': 0, 'characterOffsetEnd': 6, 'pos': 'NN', 'before': '', 'after': ' '}, {'index': 2, 'word': 'is', 'originalText': 'is', 'lemma': 'be', 'characterOffsetBegin': 7, 'characterOffsetEnd': 9, 'pos': 'VBZ', 'before': ' ', 'after': ' '}, {'index': 3, 'word': 'providing', 'originalText': 'providing', 'lemma': 'provide', 'characterOffsetBegin': 10, 'characterOffsetEnd': 19, 'pos': 'VBG', 'before': ' ', 'after': ' '}, {'index': 4, 'word': 'a', 'originalText': 'a', 'lemma': 'a', 'characterOffsetBegin': 20, 'characterOffsetEnd': 21, 'pos': 'DT', 'before': ' ', 'after': ' '}, {'index': 5, 'word': 'couple', 'originalText': 'couple', 'lemma': 'couple', 'characterOffsetBegin': 22, 'characterOffsetEnd': 28, 'pos': 'NN', 'before': ' ', 'after': ' '}, {'index': 6, 'word': 'of', 'originalText': 'of', 'lemma': 'of', 'characterOffsetBegin': 29, 'characterOffsetEnd': 31, 'pos': 'IN', 'before': ' ', 'after': ' '}, {'index': 7, 'word': 'sample', 'originalText': 'sample', 'lemma': 'sample', 'characterOffsetBegin': 32, 'characterOffsetEnd': 38, 'pos': 'NN', 'before': ' ', 'after': ' '}, {'index': 8, 'word': 'sentences', 'originalText': 'sentences', 'lemma': 'sentence', 'characterOffsetBegin': 39, 'characterOffsetEnd': 48, 'pos': 'NNS', 'before': ' ', 'after': ''}, {'index': 9, 'word': '.', 'originalText': '.', 'lemma': '.', 'characterOffsetBegin': 48, 'characterOffsetEnd': 49, 'pos': '.', 'before': '', 'after': ''}]\n",
      "ipdb> tokens['characterOffsetBegin']\n",
      "*** TypeError: list indices must be integers or slices, not str\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(sentences):\n",
    "    tokens_all = []\n",
    "    for item in sentences:\n",
    "        try:\n",
    "            nlp_res_raw = Snlp.annotate(item['sentence'])\n",
    "            nlp_res = json.loads(nlp_res_raw)\n",
    "        except Exception as e:\n",
    "            print('[Warning] StanfordCore Exception: ', nlp_res_raw, 'This sentence will be ignored.')\n",
    "            continue\n",
    "        tokens = nlp_res['sentences'][0]['tokens']\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'index': 1,\n",
       "  'word': 'caitie',\n",
       "  'originalText': 'caitie',\n",
       "  'lemma': 'caitie',\n",
       "  'characterOffsetBegin': 0,\n",
       "  'characterOffsetEnd': 6,\n",
       "  'pos': 'NN',\n",
       "  'before': '',\n",
       "  'after': ' '},\n",
       " {'index': 2,\n",
       "  'word': 'might',\n",
       "  'originalText': 'might',\n",
       "  'lemma': 'might',\n",
       "  'characterOffsetBegin': 7,\n",
       "  'characterOffsetEnd': 12,\n",
       "  'pos': 'MD',\n",
       "  'before': ' ',\n",
       "  'after': ' '},\n",
       " {'index': 3,\n",
       "  'word': 'need',\n",
       "  'originalText': 'need',\n",
       "  'lemma': 'need',\n",
       "  'characterOffsetBegin': 13,\n",
       "  'characterOffsetEnd': 17,\n",
       "  'pos': 'VB',\n",
       "  'before': ' ',\n",
       "  'after': ' '},\n",
       " {'index': 4,\n",
       "  'word': 'some',\n",
       "  'originalText': 'some',\n",
       "  'lemma': 'some',\n",
       "  'characterOffsetBegin': 18,\n",
       "  'characterOffsetEnd': 22,\n",
       "  'pos': 'DT',\n",
       "  'before': ' ',\n",
       "  'after': ' '},\n",
       " {'index': 5,\n",
       "  'word': 'help',\n",
       "  'originalText': 'help',\n",
       "  'lemma': 'help',\n",
       "  'characterOffsetBegin': 23,\n",
       "  'characterOffsetEnd': 27,\n",
       "  'pos': 'NN',\n",
       "  'before': ' ',\n",
       "  'after': ' '},\n",
       " {'index': 6,\n",
       "  'word': 'from',\n",
       "  'originalText': 'from',\n",
       "  'lemma': 'from',\n",
       "  'characterOffsetBegin': 28,\n",
       "  'characterOffsetEnd': 32,\n",
       "  'pos': 'IN',\n",
       "  'before': ' ',\n",
       "  'after': ' '},\n",
       " {'index': 7,\n",
       "  'word': 'jewell',\n",
       "  'originalText': 'jewell',\n",
       "  'lemma': 'jewell',\n",
       "  'characterOffsetBegin': 33,\n",
       "  'characterOffsetEnd': 39,\n",
       "  'pos': 'NN',\n",
       "  'before': ' ',\n",
       "  'after': ' '},\n",
       " {'index': 8,\n",
       "  'word': 'or',\n",
       "  'originalText': 'or',\n",
       "  'lemma': 'or',\n",
       "  'characterOffsetBegin': 40,\n",
       "  'characterOffsetEnd': 42,\n",
       "  'pos': 'CC',\n",
       "  'before': ' ',\n",
       "  'after': ' '},\n",
       " {'index': 9,\n",
       "  'word': 'max',\n",
       "  'originalText': 'max',\n",
       "  'lemma': 'max',\n",
       "  'characterOffsetBegin': 43,\n",
       "  'characterOffsetEnd': 46,\n",
       "  'pos': 'NN',\n",
       "  'before': ' ',\n",
       "  'after': ''},\n",
       " {'index': 10,\n",
       "  'word': '.',\n",
       "  'originalText': '.',\n",
       "  'lemma': '.',\n",
       "  'characterOffsetBegin': 46,\n",
       "  'characterOffsetEnd': 47,\n",
       "  'pos': '.',\n",
       "  'before': '',\n",
       "  'after': ''}]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tokens(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data_type, files):\n",
    "    result = []\n",
    "    #event_count, entity_count, sent_count, argument_count = 0, 0, 0, 0\n",
    "    \n",
    "    print('=' * 20)\n",
    "    print('[preprocessing] type: ', data_type)\n",
    "    for file in tqdm(files):\n",
    "        #parser = Parser(path=file)\n",
    "        #entity_count += len(parser.entity_mentions)\n",
    "        #event_count += len(parser.event_mentions)\n",
    "        #sent_count += len(parser.sents_with_pos)\n",
    "\n",
    "        for item in sentences:\n",
    "            data = dict()\n",
    "            data['sentence'] = item['sentence']\n",
    "            data['golden-entity-mentions'] = []\n",
    "            data['golden-event-mentions'] = []\n",
    "\n",
    "            try:\n",
    "                nlp_res_raw = Snlp.annotate(item['sentence'])\n",
    "                nlp_res = json.loads(nlp_res_raw)\n",
    "            except Exception as e:\n",
    "                print('[Warning] StanfordCore Exception: ', nlp_res_raw, 'This sentence will be ignored.')\n",
    "                print('If you want to include all sentences, please refer to this issue: https://github.com/nlpcl-lab/ace2005-preprocessing/issues/1')\n",
    "                continue\n",
    "\n",
    "            tokens = nlp_res['sentences'][0]['tokens']\n",
    "\n",
    "            if len(nlp_res['sentences']) >= 2:\n",
    "                # TODO: issue where the sentence segmentation of NTLK and StandfordCoreNLP do not match\n",
    "                # This error occurred so little that it was temporarily ignored (< 20 sentences).\n",
    "                continue\n",
    "\n",
    "            data['stanford-colcc'] = []\n",
    "            for dep in nlp_res['sentences'][0]['enhancedPlusPlusDependencies']:\n",
    "                data['stanford-colcc'].append('{}/dep={}/gov={}'.format(dep['dep'], dep['dependent'] - 1, dep['governor'] - 1))\n",
    "\n",
    "            data['words'] = list(map(lambda x: x['word'], tokens))\n",
    "            data['pos-tags'] = list(map(lambda x: x['pos'], tokens))\n",
    "            data['lemma'] = list(map(lambda x: x['lemma'], tokens))\n",
    "            data['parse'] = nlp_res['sentences'][0]['parse']\n",
    "\n",
    "            sent_start_pos = item['position'][0]\n",
    "\n",
    "            for entity_mention in item['golden-entity-mentions']:\n",
    "               # position = entity_mention['position']\n",
    "                start_idx, end_idx = find_token_index(\n",
    "                    tokens=tokens,\n",
    "                    start_pos= entity_mention['start'],\n",
    "                    end_pos=entity_mention['end'],\n",
    "                    phrase=entity_mention['text'],\n",
    "                )\n",
    "\n",
    "                entity_mention['start'] = entity_mention['position'][0]\n",
    "                entity_mention['end'] = entity_mention['position'][1]\n",
    "\n",
    "               # del entity_mention['position']\n",
    "\n",
    "                data['golden-entity-mentions'].append(entity_mention)\n",
    "\n",
    "            result.append(data)\n",
    "            \n",
    "   # return result #verify_result(result)\n",
    "    with open('output/{}.json'.format(data_type), 'w') as f:\n",
    "         json.dump(result, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "[preprocessing] type:  caitie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'start'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-21022e10fe88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"caitie\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-2d15f6827f27>\u001b[0m in \u001b[0;36mpreprocessing\u001b[0;34m(data_type, files)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 start_idx, end_idx = find_token_index(\n\u001b[1;32m     48\u001b[0m                     \u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                     \u001b[0mstart_pos\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mentity_mention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                     \u001b[0mend_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mentity_mention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'end'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0mphrase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mentity_mention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'start'"
     ]
    }
   ],
   "source": [
    "preprocessing(\"caitie\", sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-42-47c24e330399>\u001b[0m(49)\u001b[0;36mpreprocessing\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     47 \u001b[0;31m                start_idx, end_idx = find_token_index(\n",
      "\u001b[0m\u001b[0;32m     48 \u001b[0;31m                    \u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 49 \u001b[0;31m                    \u001b[0mstart_pos\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mentity_mention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     50 \u001b[0;31m                    \u001b[0mend_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mentity_mention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'end'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     51 \u001b[0;31m                    \u001b[0mphrase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mentity_mention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> entity_mention\n",
      "{'text': 'caitie', 'position': [50, 56], 'entity-type': 'PERSON'}\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\r\n",
      "-rw-r--r--  1 chilv  staff  5410 Jan  6 14:48 caitie.json\r\n"
     ]
    }
   ],
   "source": [
    "ls -ltr output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
