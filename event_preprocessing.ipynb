{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "\n",
    "sys.path.append('/Users/chilv/Documents/proj-wm/event_extraction/bert-event-extraction-master/ace2005-preprocessing-master')\n",
    "\n",
    "from tqdm import tqdm\n",
    "from main import find_token_index\n",
    "from _parser import Parser\n",
    "import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg', disable=[\"tagger\", \"parser\"])\n",
    "max_length = os.getenv(\"MAX_DOCUMENT_LENGTH\")\n",
    "if max_length:\n",
    "    nlp.max_length = int(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "globbed_files = glob.glob(\"/Users/chilv/Documents/proj-wm/bias-stance/bias_stance/MITRE Six-Twelve Month and November Docs CDRs/*.cdr\")\n",
    "data = []\n",
    "for one_file in globbed_files:\n",
    "    frame = pd.read_json(one_file, lines=True)\n",
    "    data.append(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>capture_source</th>\n",
       "      <th>extracted_metadata</th>\n",
       "      <th>content_type</th>\n",
       "      <th>team</th>\n",
       "      <th>document_id</th>\n",
       "      <th>extracted_text</th>\n",
       "      <th>uri</th>\n",
       "      <th>source_uri</th>\n",
       "      <th>extracted_ntriples</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>annotations</th>\n",
       "      <th>categories</th>\n",
       "      <th>extracted_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>BackgroundSource</td>\n",
       "      <td>{'CreationDate': '2017-09-14', 'ModDate': '201...</td>\n",
       "      <td>application/pdf</td>\n",
       "      <td>Two Six Labs</td>\n",
       "      <td>ee0e47a89787f974467b5118885fdb06</td>\n",
       "      <td>GIEWS global information and early warning sys...</td>\n",
       "      <td>http://graph.causeex.com/documents/sources#ee0...</td>\n",
       "      <td>ee0e47a89787f974467b5118885fdb06.pdf</td>\n",
       "      <td>&lt;http://graph.causeex.com/documents/sources#ee...</td>\n",
       "      <td>2019-09-24 12:23:38+00:00</td>\n",
       "      <td>[{'type': 'tags', 'label': 'Qntfy Event detect...</td>\n",
       "      <td>[November 2019 SSudan Docs, Six-Month Evaluati...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>BackgroundSource</td>\n",
       "      <td>{'CreationDate': '2014-08-05', 'ModDate': '201...</td>\n",
       "      <td>application/pdf</td>\n",
       "      <td>Two Six Labs</td>\n",
       "      <td>bd5386a9044fb2a16783e60da70d74e3</td>\n",
       "      <td>GOVERNMENT OF THE REPUBLIC OF SOUTH SUDAN PART...</td>\n",
       "      <td>http://graph.causeex.com/documents/sources#bd5...</td>\n",
       "      <td>bd5386a9044fb2a16783e60da70d74e3.pdf</td>\n",
       "      <td>&lt;http://graph.causeex.com/documents/sources#bd...</td>\n",
       "      <td>2019-09-24 14:24:57+00:00</td>\n",
       "      <td>[{'type': 'tags', 'label': 'Qntfy Event detect...</td>\n",
       "      <td>[Twelve-Month Eval Docs]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>BackgroundSource</td>\n",
       "      <td>{'CreationDate': '2018-04-13', 'ModDate': '201...</td>\n",
       "      <td>application/pdf</td>\n",
       "      <td>Two Six Labs</td>\n",
       "      <td>cacfddfc1b3e72cb0a23c3b6dd3e81c4</td>\n",
       "      <td>Shelter NFI Cluster South Sudan\\n\\nShelter/NFI...</td>\n",
       "      <td>http://graph.causeex.com/documents/sources#cac...</td>\n",
       "      <td>cacfddfc1b3e72cb0a23c3b6dd3e81c4.pdf</td>\n",
       "      <td>&lt;http://graph.causeex.com/documents/sources#ca...</td>\n",
       "      <td>2019-09-24 12:23:41+00:00</td>\n",
       "      <td>[{'type': 'tags', 'label': 'Qntfy Event detect...</td>\n",
       "      <td>[November 2019 SSudan Docs]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>BackgroundSource</td>\n",
       "      <td>{'CreationDate': '2017-06-09', 'ModDate': '201...</td>\n",
       "      <td>application/pdf</td>\n",
       "      <td>Two Six Labs</td>\n",
       "      <td>0fc79680cfa986f5f0c0321d52134e3d</td>\n",
       "      <td>Abathok, Abyei Intentions Survey Intentions Su...</td>\n",
       "      <td>http://graph.causeex.com/documents/sources#0fc...</td>\n",
       "      <td>0fc79680cfa986f5f0c0321d52134e3d.pdf</td>\n",
       "      <td>&lt;http://graph.causeex.com/documents/sources#0f...</td>\n",
       "      <td>2019-09-24 12:26:57+00:00</td>\n",
       "      <td>[{'type': 'tags', 'label': 'Qntfy NER', 'versi...</td>\n",
       "      <td>[November 2019 SSudan Docs]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>BackgroundSource</td>\n",
       "      <td>{'CreationDate': '2017-04-20', 'ModDate': '201...</td>\n",
       "      <td>application/pdf</td>\n",
       "      <td>Two Six Labs</td>\n",
       "      <td>b82f4e5c37a793a08fc93a2cace2b03b</td>\n",
       "      <td>In this issue Thousands flee Jonglei clashes P...</td>\n",
       "      <td>http://graph.causeex.com/documents/sources#b82...</td>\n",
       "      <td>b82f4e5c37a793a08fc93a2cace2b03b.pdf</td>\n",
       "      <td>&lt;http://graph.causeex.com/documents/sources#b8...</td>\n",
       "      <td>2019-09-24 12:27:03+00:00</td>\n",
       "      <td>[{'type': 'tags', 'label': 'Qntfy NER', 'versi...</td>\n",
       "      <td>[November 2019 SSudan Docs]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     capture_source                                 extracted_metadata  \\\n",
       "0  BackgroundSource  {'CreationDate': '2017-09-14', 'ModDate': '201...   \n",
       "1  BackgroundSource  {'CreationDate': '2014-08-05', 'ModDate': '201...   \n",
       "2  BackgroundSource  {'CreationDate': '2018-04-13', 'ModDate': '201...   \n",
       "3  BackgroundSource  {'CreationDate': '2017-06-09', 'ModDate': '201...   \n",
       "4  BackgroundSource  {'CreationDate': '2017-04-20', 'ModDate': '201...   \n",
       "\n",
       "      content_type          team                       document_id  \\\n",
       "0  application/pdf  Two Six Labs  ee0e47a89787f974467b5118885fdb06   \n",
       "1  application/pdf  Two Six Labs  bd5386a9044fb2a16783e60da70d74e3   \n",
       "2  application/pdf  Two Six Labs  cacfddfc1b3e72cb0a23c3b6dd3e81c4   \n",
       "3  application/pdf  Two Six Labs  0fc79680cfa986f5f0c0321d52134e3d   \n",
       "4  application/pdf  Two Six Labs  b82f4e5c37a793a08fc93a2cace2b03b   \n",
       "\n",
       "                                      extracted_text  \\\n",
       "0  GIEWS global information and early warning sys...   \n",
       "1  GOVERNMENT OF THE REPUBLIC OF SOUTH SUDAN PART...   \n",
       "2  Shelter NFI Cluster South Sudan\\n\\nShelter/NFI...   \n",
       "3  Abathok, Abyei Intentions Survey Intentions Su...   \n",
       "4  In this issue Thousands flee Jonglei clashes P...   \n",
       "\n",
       "                                                 uri  \\\n",
       "0  http://graph.causeex.com/documents/sources#ee0...   \n",
       "1  http://graph.causeex.com/documents/sources#bd5...   \n",
       "2  http://graph.causeex.com/documents/sources#cac...   \n",
       "3  http://graph.causeex.com/documents/sources#0fc...   \n",
       "4  http://graph.causeex.com/documents/sources#b82...   \n",
       "\n",
       "                             source_uri  \\\n",
       "0  ee0e47a89787f974467b5118885fdb06.pdf   \n",
       "1  bd5386a9044fb2a16783e60da70d74e3.pdf   \n",
       "2  cacfddfc1b3e72cb0a23c3b6dd3e81c4.pdf   \n",
       "3  0fc79680cfa986f5f0c0321d52134e3d.pdf   \n",
       "4  b82f4e5c37a793a08fc93a2cace2b03b.pdf   \n",
       "\n",
       "                                  extracted_ntriples  \\\n",
       "0  <http://graph.causeex.com/documents/sources#ee...   \n",
       "1  <http://graph.causeex.com/documents/sources#bd...   \n",
       "2  <http://graph.causeex.com/documents/sources#ca...   \n",
       "3  <http://graph.causeex.com/documents/sources#0f...   \n",
       "4  <http://graph.causeex.com/documents/sources#b8...   \n",
       "\n",
       "                  timestamp  \\\n",
       "0 2019-09-24 12:23:38+00:00   \n",
       "1 2019-09-24 14:24:57+00:00   \n",
       "2 2019-09-24 12:23:41+00:00   \n",
       "3 2019-09-24 12:26:57+00:00   \n",
       "4 2019-09-24 12:27:03+00:00   \n",
       "\n",
       "                                         annotations  \\\n",
       "0  [{'type': 'tags', 'label': 'Qntfy Event detect...   \n",
       "1  [{'type': 'tags', 'label': 'Qntfy Event detect...   \n",
       "2  [{'type': 'tags', 'label': 'Qntfy Event detect...   \n",
       "3  [{'type': 'tags', 'label': 'Qntfy NER', 'versi...   \n",
       "4  [{'type': 'tags', 'label': 'Qntfy NER', 'versi...   \n",
       "\n",
       "                                          categories extracted_numeric  \n",
       "0  [November 2019 SSudan Docs, Six-Month Evaluati...               NaN  \n",
       "1                           [Twelve-Month Eval Docs]               NaN  \n",
       "2                        [November 2019 SSudan Docs]               NaN  \n",
       "3                        [November 2019 SSudan Docs]               NaN  \n",
       "4                        [November 2019 SSudan Docs]               NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdr_data = pd.concat(data, ignore_index = True, sort = False)\n",
    "cdr_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = cdr_data['extracted_text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "nlp.add_pipe(sentencizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"caitie is providing a couple of sample sentences. caitie might need some help from jewell or max.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for sent in doc.sents:\n",
    "    sentence_dict = {}\n",
    "    sentence_dict['sentence'] = sent.text\n",
    "    sentence_dict['position'] = [sent.start_char, sent.end_char]\n",
    "    entities = []\n",
    "    for ent in sent.ents:\n",
    "        entity_dict = {}\n",
    "        entity_dict['text'] = ent.text\n",
    "        entity_dict['position'] = [ent.start_char, ent.end_char]\n",
    "        entity_dict['entity-type'] = ent.label_\n",
    "        entities.append(entity_dict)\n",
    "    sentence_dict['golden-entity-mentions'] = entities\n",
    "    sentence_dict['golden-event-mentions'] = []\n",
    "    sentences.append(sentence_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence': 'caitie is providing a couple of sample sentences.',\n",
       "  'position': [0, 49],\n",
       "  'golden-entity-mentions': [{'text': 'caitie',\n",
       "    'position': [0, 6],\n",
       "    'entity-type': 'ORG'}],\n",
       "  'golden-event-mentions': []},\n",
       " {'sentence': 'caitie might need some help from jewell or max.',\n",
       "  'position': [50, 97],\n",
       "  'golden-entity-mentions': [{'text': 'caitie',\n",
       "    'position': [50, 56],\n",
       "    'entity-type': 'PERSON'},\n",
       "   {'text': 'jewell', 'position': [83, 89], 'entity-type': 'PERSON'},\n",
       "   {'text': 'max', 'position': [93, 96], 'entity-type': 'PERSON'}],\n",
       "  'golden-event-mentions': []}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StanfordNLP:\n",
    "    def __init__(self, host='http://localhost', port=9000):\n",
    "        self.nlp = StanfordCoreNLP(host, port=port,\n",
    "                                   timeout=60000)  # , quiet=False, logging_level=logging.DEBUG)\n",
    "        self.props = {\n",
    "            'annotators': 'tokenize,ssplit,pos,lemma,parse',\n",
    "            'pipelineLanguage': 'en',\n",
    "            'outputFormat': 'json'\n",
    "        }\n",
    "\n",
    "    def word_tokenize(self, sentence):\n",
    "        return self.nlp.word_tokenize(sentence)\n",
    "\n",
    "    def pos(self, sentence):\n",
    "        return self.nlp.pos_tag(sentence)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        return self.nlp.parse(sentence)\n",
    "\n",
    "    def annotate(self, sentence):\n",
    "        return self.nlp.annotate(sentence, properties=self.props)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokens_to_dict(_tokens):\n",
    "        tokens = defaultdict(dict)\n",
    "        for token in _tokens:\n",
    "            tokens[int(token['index'])] = {\n",
    "                'word': token['word'],\n",
    "                'lemma': token['lemma'],\n",
    "                'pos': token['pos'],\n",
    "                'ner': token['ner']\n",
    "            }\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Snlp = StanfordNLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data_type, files):\n",
    "    result = []\n",
    "    #event_count, entity_count, sent_count, argument_count = 0, 0, 0, 0\n",
    "    \n",
    "    print('=' * 20)\n",
    "    print('[preprocessing] type: ', data_type)\n",
    "    for file in tqdm(files):\n",
    "        #parser = Parser(path=file)\n",
    "        #entity_count += len(parser.entity_mentions)\n",
    "        #event_count += len(parser.event_mentions)\n",
    "        #sent_count += len(parser.sents_with_pos)\n",
    "\n",
    "        for item in sentences:\n",
    "            data = dict()\n",
    "            data['sentence'] = item['sentence']\n",
    "            data['golden-entity-mentions'] = []\n",
    "            data['golden-event-mentions'] = []\n",
    "\n",
    "            try:\n",
    "                nlp_res_raw = Snlp.annotate(item['sentence'])\n",
    "                nlp_res = json.loads(nlp_res_raw)\n",
    "            except Exception as e:\n",
    "                print('[Warning] StanfordCore Exception: ', nlp_res_raw, 'This sentence will be ignored.')\n",
    "                print('If you want to include all sentences, please refer to this issue: https://github.com/nlpcl-lab/ace2005-preprocessing/issues/1')\n",
    "                continue\n",
    "\n",
    "            tokens = nlp_res['sentences'][0]['tokens']\n",
    "\n",
    "            if len(nlp_res['sentences']) >= 2:\n",
    "                # TODO: issue where the sentence segmentation of NTLK and StandfordCoreNLP do not match\n",
    "                # This error occurred so little that it was temporarily ignored (< 20 sentences).\n",
    "                continue\n",
    "\n",
    "            data['stanford-colcc'] = []\n",
    "            for dep in nlp_res['sentences'][0]['enhancedPlusPlusDependencies']:\n",
    "                data['stanford-colcc'].append('{}/dep={}/gov={}'.format(dep['dep'], dep['dependent'] - 1, dep['governor'] - 1))\n",
    "\n",
    "            data['words'] = list(map(lambda x: x['word'], tokens))\n",
    "            data['pos-tags'] = list(map(lambda x: x['pos'], tokens))\n",
    "            data['lemma'] = list(map(lambda x: x['lemma'], tokens))\n",
    "            data['parse'] = nlp_res['sentences'][0]['parse']\n",
    "\n",
    "            sent_start_pos = item['position'][0]\n",
    "\n",
    "            for entity_mention in item['golden-entity-mentions']:\n",
    "               # position = entity_mention['position']\n",
    "                start_idx, end_idx = find_token_index(\n",
    "                    tokens=tokens,\n",
    "                    start_pos= entity_mention['start'],\n",
    "                    end_pos=entity_mention['end'],\n",
    "                    phrase=entity_mention['text'],\n",
    "                )\n",
    "\n",
    "                entity_mention['start'] = entity_mention['position'][0]\n",
    "                entity_mention['end'] = entity_mention['position'][1]\n",
    "\n",
    "               # del entity_mention['position']\n",
    "\n",
    "                data['golden-entity-mentions'].append(entity_mention)\n",
    "\n",
    "#             for event_mention in item['golden-event-mentions']:\n",
    "#                 # same event mention can be shared\n",
    "#                 event_mention = copy.deepcopy(event_mention)\n",
    "#                 position = event_mention['trigger']['position']\n",
    "#                 start_idx, end_idx = find_token_index(\n",
    "#                     tokens=tokens,\n",
    "#                     start_pos=position[0] - sent_start_pos,\n",
    "#                     end_pos=position[1] - sent_start_pos + 1,\n",
    "#                     phrase=event_mention['trigger']['text'],\n",
    "#                 )\n",
    "\n",
    "#                 event_mention['trigger']['start'] = start_idx\n",
    "#                 event_mention['trigger']['end'] = end_idx\n",
    "#                 del event_mention['trigger']['position']\n",
    "#                 del event_mention['position']\n",
    "\n",
    "#                 arguments = []\n",
    "#                 argument_count += len(event_mention['arguments'])\n",
    "#                 for argument in event_mention['arguments']:\n",
    "#                     position = argument['position']\n",
    "#                     start_idx, end_idx = find_token_index(\n",
    "#                         tokens=tokens,\n",
    "#                         start_pos=position[0] - sent_start_pos,\n",
    "#                         end_pos=position[1] - sent_start_pos + 1,\n",
    "#                         phrase=argument['text'],\n",
    "#                     )\n",
    "\n",
    "#                     argument['start'] = start_idx\n",
    "#                     argument['end'] = end_idx\n",
    "#                     del argument['position']\n",
    "\n",
    "#                     arguments.append(argument)\n",
    "\n",
    "#                 event_mention['arguments'] = arguments\n",
    "#                 data['golden-event-mentions'].append(event_mention)\n",
    "\n",
    "            result.append(data)\n",
    "            \n",
    "   # return result #verify_result(result)\n",
    "    with open('output/{}.json'.format(data_type), 'w') as f:\n",
    "         json.dump(result, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:00<00:00,  5.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "[preprocessing] type:  caitie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  5.96it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessing(\"caitie\", sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-42-47c24e330399>\u001b[0m(49)\u001b[0;36mpreprocessing\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     47 \u001b[0;31m                start_idx, end_idx = find_token_index(\n",
      "\u001b[0m\u001b[0;32m     48 \u001b[0;31m                    \u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 49 \u001b[0;31m                    \u001b[0mstart_pos\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mentity_mention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     50 \u001b[0;31m                    \u001b[0mend_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mentity_mention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'end'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     51 \u001b[0;31m                    \u001b[0mphrase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mentity_mention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> entity_mention\n",
      "{'text': 'caitie', 'position': [50, 56], 'entity-type': 'PERSON'}\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\r\n",
      "-rw-r--r--  1 chilv  staff  5410 Jan  6 14:48 caitie.json\r\n"
     ]
    }
   ],
   "source": [
    "ls -ltr output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
