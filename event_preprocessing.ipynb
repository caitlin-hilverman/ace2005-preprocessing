{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "\n",
    "sys.path.append('/Users/chilv/Documents/proj-wm/event_extraction/bert-event-extraction-master/ace2005-preprocessing-master')\n",
    "\n",
    "from tqdm import tqdm\n",
    "from main import find_token_index\n",
    "from _parser import Parser\n",
    "import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg', disable=[\"tagger\", \"parser\"])\n",
    "max_length = os.getenv(\"MAX_DOCUMENT_LENGTH\")\n",
    "if max_length:\n",
    "    nlp.max_length = int(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "globbed_files = glob.glob(\"/Users/chilv/Documents/proj-wm/bias-stance/bias_stance/MITRE Six-Twelve Month and November Docs CDRs/*.cdr\")\n",
    "data = []\n",
    "for one_file in globbed_files:\n",
    "    frame = pd.read_json(one_file, lines=True)\n",
    "    data.append(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdr_data = pd.concat(data, ignore_index = True, sort = False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = cdr_data['extracted_text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "nlp.add_pipe(sentencizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"caitie is providing a couple of sample sentences. caitie might need some help from jewell or max.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "caitie is providing a couple of sample sentences. caitie might need some help from jewell or max."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_dict_list(doc):\n",
    "    \"\"\"Returns a list of dictionaries for each sentence in a CDR.\"\"\"\n",
    "    sentences = []\n",
    "    for sent in doc.sents:\n",
    "        sentence_dict = {}\n",
    "        sentence_dict['sentence'] = sent.text\n",
    "        sentence_dict['position'] = [sent.start_char, sent.end_char]\n",
    "        entities = []\n",
    "        for ent in sent.ents:\n",
    "            entity_dict = {}\n",
    "            entity_dict['text'] = ent.text\n",
    "            entity_dict['position'] = [ent.start_char, ent.end_char]\n",
    "            entity_dict['entity-type'] = ent.label_\n",
    "            entities.append(entity_dict)\n",
    "        sentence_dict['golden-entity-mentions'] = entities\n",
    "        sentence_dict['golden-event-mentions'] = []\n",
    "        sentences.append(sentence_dict)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence': 'caitie is providing a couple of sample sentences.',\n",
       "  'position': [0, 49],\n",
       "  'golden-entity-mentions': [{'text': 'caitie',\n",
       "    'position': [0, 6],\n",
       "    'entity-type': 'ORG'}],\n",
       "  'golden-event-mentions': []},\n",
       " {'sentence': 'caitie might need some help from jewell or max.',\n",
       "  'position': [50, 97],\n",
       "  'golden-entity-mentions': [{'text': 'caitie',\n",
       "    'position': [50, 56],\n",
       "    'entity-type': 'PERSON'},\n",
       "   {'text': 'jewell', 'position': [83, 89], 'entity-type': 'PERSON'},\n",
       "   {'text': 'max', 'position': [93, 96], 'entity-type': 'PERSON'}],\n",
       "  'golden-event-mentions': []}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StanfordNLP:\n",
    "    def __init__(self, host='http://localhost', port=9000):\n",
    "        self.nlp = StanfordCoreNLP(host, port=port,\n",
    "                                   timeout=60000)  # , quiet=False, logging_level=logging.DEBUG)\n",
    "        self.props = {\n",
    "            'annotators': 'tokenize,ssplit,pos,lemma,parse',\n",
    "            'pipelineLanguage': 'en',\n",
    "            'outputFormat': 'json'\n",
    "        }\n",
    "\n",
    "    def annotate(self, sentence):\n",
    "        return self.nlp.annotate(sentence, properties=self.props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Snlp = StanfordNLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence': 'caitie is providing a couple of sample sentences.',\n",
       "  'position': [0, 49],\n",
       "  'golden-entity-mentions': [{'text': 'caitie',\n",
       "    'position': [0, 6],\n",
       "    'entity-type': 'ORG'}],\n",
       "  'golden-event-mentions': []},\n",
       " {'sentence': 'caitie might need some help from jewell or max.',\n",
       "  'position': [50, 97],\n",
       "  'golden-entity-mentions': [{'text': 'caitie',\n",
       "    'position': [50, 56],\n",
       "    'entity-type': 'PERSON'},\n",
       "   {'text': 'jewell', 'position': [83, 89], 'entity-type': 'PERSON'},\n",
       "   {'text': 'max', 'position': [93, 96], 'entity-type': 'PERSON'}],\n",
       "  'golden-event-mentions': []}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_data(sentences):\n",
    "    result = []\n",
    "    \"\"\"Returns a list of dictionaries for each sentence in a CDR.\"\"\"\n",
    "    for item in sentences:\n",
    "        data = dict()\n",
    "        data['sentence'] = item['sentence']\n",
    "        data['golden-entity-mentions'] = item['golden-entity-mentions']\n",
    "        data['golden-event-mentions'] = []\n",
    "        try:\n",
    "            nlp_res_raw = Snlp.annotate(item['sentence'])\n",
    "            nlp_res = json.loads(nlp_res_raw)\n",
    "            result.append(data)\n",
    "        except Exception as e:\n",
    "            print('[Warning] StanfordCore Exception: ', nlp_res_raw, 'This sentence will be ignored.')\n",
    "            print('If you want to include all sentences, please refer to this issue: https://github.com/nlpcl-lab/ace2005-preprocessing/issues/1')\n",
    "            continue\n",
    "        tokens = nlp_res['sentences'][0]['tokens']\n",
    "        data['stanford-colcc'] = []\n",
    "        for dep in nlp_res['sentences'][0]['enhancedPlusPlusDependencies']:\n",
    "            data['stanford-colcc'].append('{}/dep={}/gov={}'.format(dep['dep'], dep['dependent'] - 1, dep['governor'] - 1))\n",
    "\n",
    "        data['words'] = list(map(lambda x: x['word'], tokens))\n",
    "        data['pos-tags'] = list(map(lambda x: x['pos'], tokens))\n",
    "        data['lemma'] = list(map(lambda x: x['lemma'], tokens))\n",
    "        data['parse'] = nlp_res['sentences'][0]['parse']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_data(sentences);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_token_index(sentences):\n",
    "    for entity_mention in item['golden-entity-mentions']:\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    for entity_mention in item['golden-entity-mentions']:\n",
    "               # position = entity_mention['position']\n",
    "                start_idx, end_idx = find_token_index(\n",
    "                    tokens=tokens,\n",
    "                    start_pos= entity_mention['start'],\n",
    "                    end_pos=entity_mention['end'],\n",
    "                    phrase=entity_mention['text'],\n",
    "                )\n",
    "\n",
    "                entity_mention['start'] = entity_mention['position'][0]\n",
    "                entity_mention['end'] = entity_mention['position'][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(sentences):\n",
    "    for item in sentences:\n",
    "        try:\n",
    "            nlp_res_raw = Snlp.annotate(item['sentence'])\n",
    "            nlp_res = json.loads(nlp_res_raw)\n",
    "        except Exception as e:\n",
    "            print('[Warning] StanfordCore Exception: ', nlp_res_raw, 'This sentence will be ignored.')\n",
    "            continue\n",
    "        tokens = nlp_res['sentences'][0]['tokens']\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'index': 1,\n",
       "  'word': 'caitie',\n",
       "  'originalText': 'caitie',\n",
       "  'lemma': 'caitie',\n",
       "  'characterOffsetBegin': 0,\n",
       "  'characterOffsetEnd': 6,\n",
       "  'pos': 'NN',\n",
       "  'before': '',\n",
       "  'after': ' '},\n",
       " {'index': 2,\n",
       "  'word': 'might',\n",
       "  'originalText': 'might',\n",
       "  'lemma': 'might',\n",
       "  'characterOffsetBegin': 7,\n",
       "  'characterOffsetEnd': 12,\n",
       "  'pos': 'MD',\n",
       "  'before': ' ',\n",
       "  'after': ' '},\n",
       " {'index': 3,\n",
       "  'word': 'need',\n",
       "  'originalText': 'need',\n",
       "  'lemma': 'need',\n",
       "  'characterOffsetBegin': 13,\n",
       "  'characterOffsetEnd': 17,\n",
       "  'pos': 'VB',\n",
       "  'before': ' ',\n",
       "  'after': ' '},\n",
       " {'index': 4,\n",
       "  'word': 'some',\n",
       "  'originalText': 'some',\n",
       "  'lemma': 'some',\n",
       "  'characterOffsetBegin': 18,\n",
       "  'characterOffsetEnd': 22,\n",
       "  'pos': 'DT',\n",
       "  'before': ' ',\n",
       "  'after': ' '},\n",
       " {'index': 5,\n",
       "  'word': 'help',\n",
       "  'originalText': 'help',\n",
       "  'lemma': 'help',\n",
       "  'characterOffsetBegin': 23,\n",
       "  'characterOffsetEnd': 27,\n",
       "  'pos': 'NN',\n",
       "  'before': ' ',\n",
       "  'after': ' '},\n",
       " {'index': 6,\n",
       "  'word': 'from',\n",
       "  'originalText': 'from',\n",
       "  'lemma': 'from',\n",
       "  'characterOffsetBegin': 28,\n",
       "  'characterOffsetEnd': 32,\n",
       "  'pos': 'IN',\n",
       "  'before': ' ',\n",
       "  'after': ' '},\n",
       " {'index': 7,\n",
       "  'word': 'jewell',\n",
       "  'originalText': 'jewell',\n",
       "  'lemma': 'jewell',\n",
       "  'characterOffsetBegin': 33,\n",
       "  'characterOffsetEnd': 39,\n",
       "  'pos': 'NN',\n",
       "  'before': ' ',\n",
       "  'after': ' '},\n",
       " {'index': 8,\n",
       "  'word': 'or',\n",
       "  'originalText': 'or',\n",
       "  'lemma': 'or',\n",
       "  'characterOffsetBegin': 40,\n",
       "  'characterOffsetEnd': 42,\n",
       "  'pos': 'CC',\n",
       "  'before': ' ',\n",
       "  'after': ' '},\n",
       " {'index': 9,\n",
       "  'word': 'max',\n",
       "  'originalText': 'max',\n",
       "  'lemma': 'max',\n",
       "  'characterOffsetBegin': 43,\n",
       "  'characterOffsetEnd': 46,\n",
       "  'pos': 'NN',\n",
       "  'before': ' ',\n",
       "  'after': ''},\n",
       " {'index': 10,\n",
       "  'word': '.',\n",
       "  'originalText': '.',\n",
       "  'lemma': '.',\n",
       "  'characterOffsetBegin': 46,\n",
       "  'characterOffsetEnd': 47,\n",
       "  'pos': '.',\n",
       "  'before': '',\n",
       "  'after': ''}]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tokens(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data_type, files):\n",
    "    result = []\n",
    "    #event_count, entity_count, sent_count, argument_count = 0, 0, 0, 0\n",
    "    \n",
    "    print('=' * 20)\n",
    "    print('[preprocessing] type: ', data_type)\n",
    "    for file in tqdm(files):\n",
    "        #parser = Parser(path=file)\n",
    "        #entity_count += len(parser.entity_mentions)\n",
    "        #event_count += len(parser.event_mentions)\n",
    "        #sent_count += len(parser.sents_with_pos)\n",
    "\n",
    "        for item in sentences:\n",
    "            data = dict()\n",
    "            data['sentence'] = item['sentence']\n",
    "            data['golden-entity-mentions'] = []\n",
    "            data['golden-event-mentions'] = []\n",
    "\n",
    "            try:\n",
    "                nlp_res_raw = Snlp.annotate(item['sentence'])\n",
    "                nlp_res = json.loads(nlp_res_raw)\n",
    "            except Exception as e:\n",
    "                print('[Warning] StanfordCore Exception: ', nlp_res_raw, 'This sentence will be ignored.')\n",
    "                print('If you want to include all sentences, please refer to this issue: https://github.com/nlpcl-lab/ace2005-preprocessing/issues/1')\n",
    "                continue\n",
    "\n",
    "            tokens = nlp_res['sentences'][0]['tokens']\n",
    "\n",
    "            if len(nlp_res['sentences']) >= 2:\n",
    "                # TODO: issue where the sentence segmentation of NTLK and StandfordCoreNLP do not match\n",
    "                # This error occurred so little that it was temporarily ignored (< 20 sentences).\n",
    "                continue\n",
    "\n",
    "            data['stanford-colcc'] = []\n",
    "            for dep in nlp_res['sentences'][0]['enhancedPlusPlusDependencies']:\n",
    "                data['stanford-colcc'].append('{}/dep={}/gov={}'.format(dep['dep'], dep['dependent'] - 1, dep['governor'] - 1))\n",
    "\n",
    "            data['words'] = list(map(lambda x: x['word'], tokens))\n",
    "            data['pos-tags'] = list(map(lambda x: x['pos'], tokens))\n",
    "            data['lemma'] = list(map(lambda x: x['lemma'], tokens))\n",
    "            data['parse'] = nlp_res['sentences'][0]['parse']\n",
    "\n",
    "            sent_start_pos = item['position'][0]\n",
    "\n",
    "            for entity_mention in item['golden-entity-mentions']:\n",
    "               # position = entity_mention['position']\n",
    "                start_idx, end_idx = find_token_index(\n",
    "                    tokens=tokens,\n",
    "                    start_pos= entity_mention['start'],\n",
    "                    end_pos=entity_mention['end'],\n",
    "                    phrase=entity_mention['text'],\n",
    "                )\n",
    "\n",
    "                entity_mention['start'] = entity_mention['position'][0]\n",
    "                entity_mention['end'] = entity_mention['position'][1]\n",
    "\n",
    "               # del entity_mention['position']\n",
    "\n",
    "                data['golden-entity-mentions'].append(entity_mention)\n",
    "\n",
    "            result.append(data)\n",
    "            \n",
    "   # return result #verify_result(result)\n",
    "    with open('output/{}.json'.format(data_type), 'w') as f:\n",
    "         json.dump(result, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "[preprocessing] type:  caitie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'start'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-21022e10fe88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"caitie\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-2d15f6827f27>\u001b[0m in \u001b[0;36mpreprocessing\u001b[0;34m(data_type, files)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 start_idx, end_idx = find_token_index(\n\u001b[1;32m     48\u001b[0m                     \u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                     \u001b[0mstart_pos\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mentity_mention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                     \u001b[0mend_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mentity_mention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'end'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0mphrase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mentity_mention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'start'"
     ]
    }
   ],
   "source": [
    "preprocessing(\"caitie\", sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-42-47c24e330399>\u001b[0m(49)\u001b[0;36mpreprocessing\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     47 \u001b[0;31m                start_idx, end_idx = find_token_index(\n",
      "\u001b[0m\u001b[0;32m     48 \u001b[0;31m                    \u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 49 \u001b[0;31m                    \u001b[0mstart_pos\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mentity_mention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     50 \u001b[0;31m                    \u001b[0mend_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mentity_mention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'end'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     51 \u001b[0;31m                    \u001b[0mphrase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mentity_mention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> entity_mention\n",
      "{'text': 'caitie', 'position': [50, 56], 'entity-type': 'PERSON'}\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\r\n",
      "-rw-r--r--  1 chilv  staff  5410 Jan  6 14:48 caitie.json\r\n"
     ]
    }
   ],
   "source": [
    "ls -ltr output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
